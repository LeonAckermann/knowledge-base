202211291045
Status: #to-revisit
Tags: [[Deep Learning]], [[AI Winter]], [[Deep Learning Revolution]]

# The resurrection of Deep Learning
It is interesting to see, why especially in 2012 AlexNet, ImageNet, etc. had such a huge success with their approach. It was not exactly new, to use Deep Learning and convolutions, so why did it work this time? My guess would be, that around that time enough computational power and data was available to make their approach work. Since then, Deep Learning is its own research field and there is many many different network classes. However the question remains if this approach will bring us to the goal of a general AI because [[Deep Learning needs data]]. This is fundamentally different from how humans learn, which is rich inferencing with sparse data. Humans understand a core concept, e.g. how a letter is written, and can reproduce this concept by writing, recognize it and more. Deep learning systems on the other hand are not capable of making inferences with the same sparse data as humans. When not provided data explicitely in the data, networks are not able to generalize at all outside their given data, as seen in [[marcusDeepLearningCritical 1]] with the identity function. 


# References
[[marcusDeepLearningCritical 1]]
[[lakeBuildingMachinesThat2016]]
